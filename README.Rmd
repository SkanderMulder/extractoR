---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# instructoR <img src="man/figures/logo.png" align="right" height="114" alt="" />

<!-- badges: start -->
<!-- badges: end -->

> **Self-correcting structured extraction from text using LLMs in R**

Like Python's [instructor](https://github.com/jxnl/instructor) or [outlines](https://github.com/outlines-dev/outlines), but native to R. No more fighting with malformed JSON or parsing errors—instructoR automatically retries LLM calls with validation feedback until the output perfectly matches your schema.

## Why instructoR?

When working with LLMs for data extraction, you typically face:

- **Malformed JSON** that breaks `jsonlite::fromJSON()`
- **Schema violations** like wrong types or missing fields
- **Manual retry logic** with hacky error handling
- **Unreliable parsing** that fails in production

instructoR solves this by:

1. Converting R schemas to JSON Schema automatically
2. Validating LLM responses against your schema
3. Providing intelligent feedback to the LLM when validation fails
4. Retrying with self-correction until success or max attempts

**Result:** Guaranteed valid output or controlled failure. No surprises.

## Installation

Install from GitHub:

```r
# install.packages("pak")
pak::pak("SkanderMulder/extractoR")
```

## Quick Start

### Basic Usage

Extract structured data from unstructured text:

```{r eval=FALSE}
library(instructoR)

# Define what you want to extract
schema <- list(
  sentiment = c("positive", "negative", "neutral"),
  confidence = "numeric",
  keywords = list("character"),
  entities = list(list(
    name = "character",
    type = "character"
  ))
)

# Extract from text
review <- "Amazing product! Fast shipping and great customer service.
          Apple delivered another winner with the iPhone."

result <- extract(
  text = review,
  schema = schema,
  model = "gpt-4o-mini"
)

str(result)
# List of 4
#  $ sentiment : chr "positive"
#  $ confidence: num 0.95
#  $ keywords  : List of 3
#  $ entities  : List of 2
```

### Local Models with Ollama

Use local LLMs for privacy and cost savings:

```{r eval=FALSE}
# Start Ollama: ollama serve
# Pull a model: ollama pull gemma:2b

result <- extract(
  text = article_text,
  schema = schema,
  model = "ollama/gemma:2b"
)
```

See `inst/examples/ollama_example.R` for a complete working example.

## Features

### Self-Correction Strategies

Choose how the LLM receives feedback:

```{r eval=FALSE}
# Reflective: provides context and encourages reasoning
extract(text, schema, strategy = "reflect")

# Direct: concise error feedback
extract(text, schema, strategy = "direct")

# Polite: friendly tone for models that respond well to courtesy
extract(text, schema, strategy = "polite")
```

### Rich Schema Support

Define complex nested structures naturally in R:

```{r eval=FALSE}
schema <- list(
  # Enums (controlled vocabularies)
  status = c("draft", "published", "archived"),

  # Basic types
  title = "character",
  views = "integer",
  rating = "numeric",
  is_featured = "logical",

  # Arrays
  tags = list("character"),

  # Nested objects
  author = list(
    name = "character",
    email = "character"
  ),

  # Arrays of objects
  comments = list(list(
    user = "character",
    text = "character",
    timestamp = "character"
  ))
)
```

### Progress Tracking

Monitor extraction with built-in progress indicators:

```{r eval=FALSE}
extract(text, schema, .progress = TRUE)
# ── Starting Extraction ──────────────────────────
# ℹ Model: gpt-4o-mini
# ℹ Max retries: 5
# ℹ Strategy: reflect
# ℹ Sending initial request to LLM...
# ℹ Received initial response. Validating...
# ✔ Extraction successful after 1 attempts.
```

## Real-World Examples

### Academic Paper Metadata

```{r eval=FALSE}
paper_schema <- list(
  title = "character",
  authors = list("character"),
  year = "integer",
  abstract = "character",
  topics = list("character"),
  methodology = c("qualitative", "quantitative", "mixed"),
  is_open_access = "logical"
)

extract(paper_text, paper_schema, model = "gpt-4o-mini")
```

### Product Review Analysis

```{r eval=FALSE}
review_schema <- list(
  overall_sentiment = c("positive", "negative", "neutral"),
  rating_estimate = "integer",
  pros = list("character"),
  cons = list("character"),
  mentioned_features = list(list(
    feature = "character",
    sentiment = c("positive", "negative", "neutral")
  ))
)

extract(review_text, review_schema, model = "gpt-4o-mini")
```

### Clinical Notes Extraction

```{r eval=FALSE}
clinical_schema <- list(
  patient_age = "integer",
  chief_complaint = "character",
  symptoms = list("character"),
  diagnoses = list(list(
    condition = "character",
    icd_code = "character",
    confidence = c("definite", "probable", "possible")
  )),
  medications = list(list(
    name = "character",
    dosage = "character",
    frequency = "character"
  ))
)

extract(clinical_note, clinical_schema, model = "gpt-4o-mini")
```

## How It Works

1. **Schema Conversion**: Your R list schema is converted to JSON Schema
2. **Initial Extraction**: The LLM receives your text and schema requirements
3. **Validation**: Response is validated against JSON Schema using `jsonvalidate`
4. **Self-Correction Loop**: If invalid, the LLM gets error feedback and retries
5. **Guaranteed Output**: Returns valid structured data or fails gracefully

```
Text + Schema → LLM → Validate → ✓ Success
                         ↓
                      ✗ Failed
                         ↓
                   Feedback → LLM → Validate → ...
```

## Comparison with Alternatives

| Feature | instructoR | Manual Parsing | Python instructor |
|---------|-----------|----------------|-------------------|
| Automatic retry | ✓ | ✗ | ✓ |
| Schema validation | ✓ | ✗ | ✓ |
| Native R types | ✓ | Partial | ✗ |
| Local models | ✓ | ✓ | ✓ |
| Zero dependencies* | ✓ | ✓ | ✗ |

*Minimal dependencies: only essential R packages

## Configuration

### Set up API keys

For OpenAI/Anthropic models via ellmer:

```{r eval=FALSE}
# In .Renviron:
ANTHROPIC_API_KEY="your-key-here"
OPENAI_API_KEY="your-key-here"
```

### Customize retry behavior

```{r eval=FALSE}
extract(
  text = text,
  schema = schema,
  model = "gpt-4o-mini",
  max_retries = 10,        # Default: 5
  temperature = 0.0,       # Default: 0.0 (deterministic)
  strategy = "reflect"     # Default: "reflect"
)
```

## Learn More

- **Vignette**: `vignette("getting-started", package = "instructoR")`
- **Examples**: See `inst/examples/ollama_example.R`
- **Documentation**: `?extract`
- **Issues**: [GitHub Issues](https://github.com/SkanderMulder/extractoR/issues)

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

MIT License. See LICENSE for details.

## Acknowledgments

Inspired by:
- [instructor](https://github.com/jxnl/instructor) (Python)
- [outlines](https://github.com/outlines-dev/outlines) (Python)
- [ellmer](https://github.com/hadley/ellmer) (R LLM client)
